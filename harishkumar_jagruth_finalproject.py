# -*- coding: utf-8 -*-
"""harishkumar_jagruth_finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I0cldDyfqRtCOh0nLJjleVTclmxgr3ar
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px #
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import accuracy_score
from termcolor import colored
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from IPython.display import display
print(colored("SUCCESSFULLY IMPORTED...", "green", attrs=['reverse']))

vino = pd.read_csv('winequality-red.csv')
vino.describe()

vino.info()

print(vino.shape)

print("The shape of the dataset=",vino.shape)

nrow, ncols = vino.shape
nf = ncols - 1
totdata = nrow * ncols

print(f"total number of Rows: {nrow}")
print(f"total number of Columns: {ncols}")
print(f"total number of Features: {nf}")
print(f"total data: {totdata}")

#Displaying the first 10 rows.
dataset_rows = vino.head(10)
print(colored('As you can see, the first 7 rows in the dataset:\n', 'green', attrs=['reverse']))

# Iterates over each row and displayes them
for index, row in dataset_rows.iterrows():
    print(colored(f"Row {index + 1}:",attrs=['reverse']))
    print(row)
    print("-----------------x--------------------")

print(vino.info())

vino.describe().T.round(2)

sns.catplot(data=vino, x='quality', kind='count')

plt.title('Wine Quality distribution graph')
plt.xlabel('Quality')
plt.ylabel('Count')
plt.show()

fig, ax = plt.subplots(figsize=(10, 5))
sns.barplot(x='quality', y='volatile acidity', data=vino, ax=ax)
#plotting the bar graph for acidity
# Set plot title and axis labels
ax.set_title('Volatile Acidity of Wine by Quality')
ax.set_xlabel('Quality')
ax.set_ylabel('Volatile Acidity')
sns.set_palette('dark')
plt.xticks(rotation=45)
plt.show()

fig, ax = plt.subplots(figsize=(8, 6))
sns.barplot(x='quality', y='alcohol', data=vino, ax=ax)
ax.set_title('alcohol by Quality')
ax.set_xlabel('Quality')
ax.set_ylabel('alcohol')
sns.set_palette('dark')
plt.xticks(rotation=45)
plt.show()

nullC = vino.isnull().sum()
print(nullC)

print(colored(f"Totally, there are {nullC.sum()} null values in the dataset."))

# Graph to represent outliers
plt.figure(figsize=(21, 10))

sns.stripplot(data=vino, color="red", jitter=0.2, size=5)
plt.title("Outliers")
plt.xlabel("X-axis label")
plt.ylabel("Y-axis label")
plt.show()

# Deleting outliers (Removing the number of observation where the total sulfur dioxide is more than 160)

print("Before Removing the outliers", vino.shape)
data = vino[vino['total sulfur dioxide']<160]

#The data after deleting outliers
print("After Removing the outliers", vino.shape)

corr = data.corr()
# cmap = sns.diverging_palette(-1, 1, s=100, l=50, n=15, center="dark", as_cmap=True)
plt.figure(figsize=(9, 6))
sns.heatmap(corr, annot=True, fmt='.2f', linewidth=0.5, cmap='Purples', mask=np.triu(corr))
plt.show()

plt.figure(figsize=(22, 11))

sns.boxplot(data=vino)
plt.xlabel("X-axis label")
plt.ylabel("Y-axis label")
plt.title("Boxplot")
plt.show()

# Checking for duplicate entries
duplicate_count = vino.duplicated().sum()
if duplicate_count == 0:
    print(("No duplicate entries found in the dataset."))
else:
    print((f"Number of duplicate entries found: {duplicate_count}"))

vino.hist(figsize=(10, 10))
plt.suptitle('Histogram of Each Numeric Column')
plt.show()

# Plot pairwise relationships in the 'data' DataFrame and color the points based on the 'quality' variable and then display the plot
sns.pairplot(vino, hue="quality")
plt.show()

vino.quality.value_counts()

# features and values are seperated. If quality is greater than  then it is to be taken as good quality wine
X = vino.iloc[:, :-1].values  # Features
y = (vino.iloc[:, -1].values > 5).astype(int)

# Normalize the features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Calculate metrics
def metricCalc(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    TPR = tp / (tp + fn) if (tp + fn) > 0 else 0
    TNR = tn / (tn + fp) if (tn + fp) > 0 else 0
    FPR = fp / (fp + tn) if (fp + tn) > 0 else 0
    FNR = fn / (fn + tp) if (fn + tp) > 0 else 0
    Precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    F1Score = 2 * (Precision * TPR) / (Precision + TPR) if (Precision + TPR) > 0 else 0
    errorRate = (fp + fn) / (tp + tn + fp + fn)
    BACC = (TPR + TNR) / 2
    TSS = TPR - FPR
    HSS = 2 * (tp * tn - fp * fn) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn)) if (tp + fn) * (fn + tn) + (tp + fp) * (fp + tn) > 0 else 0
    return {
        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,
        'TPR': TPR, 'TNR': TNR, 'FPR': FPR, 'FNR': FNR,
        'Precision': Precision, 'F1Score': F1Score,
        'errorRate': errorRate, 'BACC': BACC, 'TSS': TSS, 'HSS': HSS
    }

# 10 fold Cross-validation
def validation(model, mtype, X, y, n_splits=10):
    kf = KFold(n_splits =n_splits , shuffle=True, random_state=42)
    metrics_list = []

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        if mtype == 'LSTM':
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
            y_pred = (model.predict(X_test) > 0.5).astype(int)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

        metrics = metricCalc(y_test, y_pred)
        metrics_list.append(metrics)

    avg_metrics = {key: np.mean([m[key] for m in metrics_list]) for key in metrics_list[0]}
    return metrics_list, avg_metrics

# Create models
def lstmMaker(input_shape):
    model = Sequential()
    model.add(LSTM(50, input_shape=input_shape, activation='tanh'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=5)
lstm_model = lstmMaker((X.shape[1], 1))

rf_metrics, rf_avg = validation(rf_model, 'RandomForest', X, y)
knn_metrics, knn_avg = validation(knn_model, 'KNN', X, y)
lstm_metrics, lstm_avg = validation(lstm_model, 'LSTM', X, y)
#tabulated results
def tableResults(metrics_list, avg_metrics):
    df = pd.DataFrame(metrics_list)
    avg_row = pd.DataFrame(avg_metrics, index=["Average"])
    return pd.concat([df, avg_row])

rf_results = tableResults(rf_metrics, rf_avg)
knn_results = tableResults(knn_metrics, knn_avg)
lstm_results = tableResults(lstm_metrics, lstm_avg)

print("Random Forest Metrics:")
display(rf_results)

print("KNN Metrics:")
display(knn_results)

print("LSTM Metrics:")
display(lstm_results)

def rocCurve(models, model_names, X, y):
    plt.figure(figsize=(10, 6))
    for model, name in zip(models, model_names):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        if name == "LSTM":
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
            y_proba = model.predict(X_test).ravel()
        else:
            model.fit(X_train, y_train)
            y_proba = model.predict_proba(X_test)[:, 1]

        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

    plt.plot([0, 1], [0, 1], 'k--', label="Random Chance")
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.show()

rocCurve([rf_model, knn_model, lstm_model], ["Random Forest", "KNN", "LSTM"], X, y)

# Discussion
print("Discussion:")
if rf_avg["F1Score"] > knn_avg["F1Score"] and rf_avg["F1Score"] > lstm_avg["F1Score"]:
    print("Random Forest performs the best due to its ensemble nature, reducing overfitting and handling complex interactions.")
elif knn_avg["F1Score"] > rf_avg["F1Score"] and knn_avg["F1Score"] > lstm_avg["F1Score"]:
    print("KNN performs the best as the dataset's local structure suits proximity-based methods.")
else:
    print("LSTM performs the best, leveraging temporal and complex feature relationships due to its deep learning architecture.")